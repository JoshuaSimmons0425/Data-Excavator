{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "803c457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from scipy.stats import skew\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting the dataset\n",
    "\n",
    "def create_sets(df, train_temp_ratio, val_test_ratio, target):\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "    \n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=train_temp_ratio, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size = val_test_ratio, random_state = 42)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "# Missing Values inspection\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_counts = missing_counts[missing_counts > 0]\n",
    "    if missing_counts.empty:\n",
    "        print(\"No missing values found!\")\n",
    "    else:\n",
    "        for col, count in missing_counts.items():\n",
    "            percent = 100 * count / len(df)\n",
    "            print(f\"{col}: {count} missing values ({percent:.2f}%)\")\n",
    "        print(f\"Total missing values: {missing_counts.sum()}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Numerical + Categorical Imputation\n",
    "\n",
    "def fit_impute_data(train_df, n_neighbors=2):\n",
    "    numerical_features = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = train_df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "    imputed_train = train_df.copy()\n",
    "\n",
    "    # Numerical Imputer\n",
    "    knn_imputer = None\n",
    "    if numerical_features:\n",
    "        knn_imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "        imputed_numerical = knn_imputer.fit_transform(imputed_train[numerical_features])\n",
    "        imputed_train[numerical_features] = pd.DataFrame(\n",
    "            imputed_numerical, columns=numerical_features, index=imputed_train.index\n",
    "        )\n",
    "\n",
    "    # Categorical Imputer (mode)\n",
    "    cat_imputers = {}\n",
    "    for col in categorical_features:\n",
    "        mode_val = imputed_train[col].mode()[0] if not imputed_train[col].mode().empty else \"Unknown\"\n",
    "        imputed_train[col] = imputed_train[col].fillna(mode_val)\n",
    "        cat_imputers[col] = mode_val\n",
    "\n",
    "    print(\"Imputations fitted and applied (numerical → KNN, categorical → mode)\")\n",
    "    return knn_imputer, cat_imputers, imputed_train\n",
    "\n",
    "\n",
    "def transform_impute_data(test_df, knn_imputer, cat_imputers):\n",
    "    imputed_test = test_df.copy()\n",
    "\n",
    "    # Numerical\n",
    "    numerical_features = test_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if knn_imputer is not None and numerical_features:\n",
    "        imputed_numerical = knn_imputer.transform(imputed_test[numerical_features])\n",
    "        imputed_test[numerical_features] = pd.DataFrame(\n",
    "            imputed_numerical, columns=numerical_features, index=imputed_test.index\n",
    "        )\n",
    "\n",
    "    # Categorical\n",
    "    for col, mode_val in cat_imputers.items():\n",
    "        imputed_test[col] = imputed_test[col].fillna(mode_val)\n",
    "\n",
    "    print(\"Imputations applied to test set\")\n",
    "    return imputed_test\n",
    "\n",
    "\n",
    "\n",
    "# Feature Construction (row-wise, safe before split)\n",
    "\n",
    "def feature_construct(df, transformations):\n",
    "    operations = {\n",
    "        'add': lambda cols: cols[0] + cols[1],\n",
    "        'sub': lambda cols: cols[0] - cols[1],\n",
    "        'mul': lambda cols: cols[0] * cols[1],\n",
    "        'div': lambda cols: cols[0] / cols[1],\n",
    "        'mean': lambda cols: sum(cols) / len(cols),\n",
    "        'max': lambda cols: pd.concat(cols, axis=1).max(axis=1),\n",
    "        'min': lambda cols: pd.concat(cols, axis=1).min(axis=1),\n",
    "    }\n",
    "\n",
    "    for t in transformations:\n",
    "        new_feature = t['new_feature']\n",
    "        columns = t['columns']\n",
    "        operation = t['operation']\n",
    "\n",
    "        col_data = [df[col] for col in columns]\n",
    "        if callable(operation):\n",
    "            df[new_feature] = operation(col_data)\n",
    "        elif isinstance(operation, str) and operation in operations:\n",
    "            df[new_feature] = operations[operation](col_data)\n",
    "\n",
    "    print(\"New features successfully constructed\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Adaptive Transform (fit/transform)\n",
    "\n",
    "def fit_adaptive_transform(train_df, skew_threshold=0.75, log1p_pos_threshold=0.9):\n",
    "    transformers = {}\n",
    "    transformed_train = train_df.copy()\n",
    "\n",
    "    numerical_features = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'isFraud' in numerical_features:\n",
    "        numerical_features.remove('isFraud')\n",
    "\n",
    "    for feature in numerical_features:\n",
    "        feature_data = transformed_train[feature].dropna()\n",
    "        feature_skew = skew(feature_data)\n",
    "        positive_ratio = (transformed_train[feature] > 0).sum() / len(transformed_train[feature])\n",
    "\n",
    "        if abs(feature_skew) < skew_threshold:\n",
    "            transformers[feature] = None\n",
    "            continue\n",
    "\n",
    "        if feature_skew > skew_threshold and positive_ratio >= log1p_pos_threshold:\n",
    "            transformed_train[feature] = np.log1p(transformed_train[feature])\n",
    "            transformers[feature] = \"log1p\"\n",
    "        else:\n",
    "            pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "            transformed_train[[feature]] = pt.fit_transform(transformed_train[[feature]])\n",
    "            transformers[feature] = pt\n",
    "\n",
    "    print(\"Adaptive transform fitted and applied to train\")\n",
    "    return transformers, transformed_train\n",
    "\n",
    "\n",
    "def transform_adaptive_transform(test_df, transformers):\n",
    "    transformed_test = test_df.copy()\n",
    "    for feature, transformer in transformers.items():\n",
    "        if transformer is None:\n",
    "            continue\n",
    "        elif transformer == \"log1p\":\n",
    "            transformed_test[feature] = np.log1p(transformed_test[feature])\n",
    "        else:\n",
    "            transformed_test[[feature]] = transformer.transform(transformed_test[[feature]])\n",
    "    print(\"Adaptive transform applied to test\")\n",
    "    return transformed_test\n",
    "\n",
    "\n",
    "# Sampling\n",
    "\n",
    "def create_sample_size(df, fraud_ratio, sample_ratio):\n",
    "    total_sample_size = int(len(df) * sample_ratio)\n",
    "    n_fraud = int(total_sample_size * fraud_ratio)\n",
    "    n_nonfraud = total_sample_size - n_fraud\n",
    "\n",
    "    class1 = df[df['isFraud'] == 1]\n",
    "    class0 = df[df['isFraud'] == 0]\n",
    "\n",
    "    n_fraud = min(n_fraud, len(class1))\n",
    "    n_nonfraud = min(n_nonfraud, len(class0))\n",
    "\n",
    "    sample1 = class1.sample(n=n_fraud, random_state=42, replace=False)\n",
    "    sample0 = class0.sample(n=n_nonfraud, random_state=42, replace=False)\n",
    "\n",
    "    sampled_df = pd.concat([sample1, sample0])\n",
    "    sampled_df = shuffle(sampled_df, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Sampled dataset size: {len(sampled_df)} (target was {total_sample_size})\")\n",
    "    print(f\"Fraud ratio in sample: {sampled_df['isFraud'].mean():.4f}\")\n",
    "    return sampled_df\n",
    "\n",
    "# Dropping features\n",
    "\n",
    "def drop_features(df, irrel_features):\n",
    "    df = df.drop(irrel_features, axis=1, errors='ignore')\n",
    "    print(\"Dropping of features successful\")\n",
    "    return df\n",
    "\n",
    "# Encoding categorical features (fit/transform)\n",
    "\n",
    "def fit_encode_cat(train_df, cat_features):\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    train_df = train_df.copy()\n",
    "    \n",
    "    # Get all unique categories from the training data for each feature\n",
    "    all_categories = {}\n",
    "    for feature in cat_features:\n",
    "        all_categories[feature] = sorted(train_df[feature].astype('category').unique())\n",
    "    \n",
    "    # Perform one-hot encoding\n",
    "    encoded_train = pd.get_dummies(train_df, columns=cat_features, dtype=int)\n",
    "    \n",
    "    # Ensure correct column order for consistency\n",
    "    train_encoded_df = encoded_train.reindex(columns=encoded_train.columns)\n",
    "    \n",
    "    print(\"Categorical encoding applied to train\")\n",
    "    \n",
    "    return all_categories, train_encoded_df\n",
    "\n",
    "\n",
    "def transform_encode_cat(test_df, cat_features, all_categories):\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    # Perform one-hot encoding on the test set\n",
    "    encoded_test = pd.get_dummies(test_df, columns=cat_features, dtype=int)\n",
    "    \n",
    "    # Create a list of all expected columns based on training categories\n",
    "    expected_columns = []\n",
    "    for feature, categories in all_categories.items():\n",
    "        for category in categories:\n",
    "            expected_columns.append(f\"{feature}_{category}\")\n",
    "    \n",
    "    # Reindex the encoded test dataframe to align with the training columns\n",
    "    # and fill any missing columns with zeros for unseen categories\n",
    "    test_encoded_df = encoded_test.reindex(columns=expected_columns, fill_value=0)\n",
    "    \n",
    "    # Preserve the original non-categorical columns\n",
    "    non_cat_columns = [col for col in test_df.columns if col not in cat_features]\n",
    "    test_encoded_df = pd.concat([test_df[non_cat_columns], test_encoded_df], axis=1)\n",
    "\n",
    "    print(\"Categorical encoding applied to test set\")\n",
    "    \n",
    "    return test_encoded_df\n",
    "\n",
    "\n",
    "def normalize_x(X_train, X_val, X_test):\n",
    "    # Create copies to prevent modifying the original dataframes\n",
    "    X_train_normalized = X_train.copy()\n",
    "    X_val_normalized = X_val.copy()\n",
    "    X_test_normalized = X_test.copy()\n",
    "\n",
    "    # Identify numeric columns for scaling\n",
    "    numeric_cols = [\n",
    "        col for col in X_train.columns\n",
    "        if not (X_train[col].dtype.kind in 'i' and X_train[col].nunique() == 2)\n",
    "    ]\n",
    "\n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit and transform the training data, then transform the validation and test data\n",
    "    X_train_normalized[numeric_cols] = scaler.fit_transform(X_train_normalized[numeric_cols])\n",
    "    X_val_normalized[numeric_cols] = scaler.transform(X_val_normalized[numeric_cols])\n",
    "    X_test_normalized[numeric_cols] = scaler.transform(X_test_normalized[numeric_cols])\n",
    "\n",
    "    print(\"Normalization applied on numeric features successful\")\n",
    "\n",
    "    return X_train_normalized, X_val_normalized, X_test_normalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6b712",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
