{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf81aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.evaluation_prototype import evaluate_nn_model\n",
    "from notebooks.evaluation_prototype import evaluate_baseline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create and train the baseline model and then validate their performance\n",
    "\n",
    "def train_log_model(df, max_iter, X_train, X_val, y_train, y_val):\n",
    "    model = LogisticRegression(max_iter=max_iter)\n",
    "    model.fit(X_train, y_train)\n",
    "    evaluate_baseline(model, 'Logistic Regression', X_val, y_val)\n",
    "    return model\n",
    "\n",
    "def train_svm(df, kernel, C, X_train, X_val, y_train, y_val, probability=True):\n",
    "    model = SVC(kernel=kernel, C=C)\n",
    "    model.fit(X_train, y_train)\n",
    "    evaluate_baseline(model, 'SVM', X_val, y_val)\n",
    "    return model\n",
    "\n",
    "def train_naive_bayes(df, X_train, X_val, y_train, y_val):\n",
    "    model = BernoulliNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    evaluate_baseline(model, 'Naive Bayes', X_val, y_val) \n",
    "    return model\n",
    "\n",
    "def train_knn(df, n_neighbors, X_train, X_val, y_train, y_val):\n",
    "    model = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    model.fit(X_train, y_train)\n",
    "    evaluate_baseline(model, 'k-NN', X_val, y_val)\n",
    "    return model\n",
    "\n",
    "def train_decision_tree(df, max_depth, X_train, X_val, y_train, y_val):\n",
    "    model = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    evaluate_baseline(model, 'Decision Tree', X_val, y_val)\n",
    "    return model\n",
    "\n",
    "def train_random_forest(df, n_estimators, max_depth, X_train, X_val, y_train, y_val):\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    evaluate_baseline(model, 'Random Forest', X_val, y_val)\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9198f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "    \n",
    "   # Convert to tensors\n",
    "def to_tensor(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    " \n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "def create_loaders(df, X_train, X_val, X_test, y_train, y_val, y_test, batch_size):\n",
    "    \n",
    "    # Create tensor datasets\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    validation_dataset = TensorDataset(X_val, y_val)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    # Create data loaders (shuffle training set)\n",
    "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True, drop_last = True)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle = False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, validation_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd5e9028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "activation_functions = {\n",
    "    \"relu\": nn.ReLU(),\n",
    "    \"leaky_relu\": nn.LeakyReLU(),\n",
    "    \"sigmoid\": nn.Sigmoid(),\n",
    "    \"tanh\": nn.Tanh(),\n",
    "    \"softmax\": nn.Softmax(dim=1),\n",
    "    \"elu\": nn.ELU(),\n",
    "    \"p_relu\": nn.PReLU(),\n",
    "    \"rrelu\": nn.RReLU(),\n",
    "    \"log_sigmoid\": nn.LogSigmoid,\n",
    "    \"silu\": nn.SiLU()\n",
    "}\n",
    "\n",
    "class FraudClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers, activations):\n",
    "        super(FraudClassifier, self).__init__()\n",
    "        # Input validation\n",
    "        if len(hidden_layers) != len(activations):\n",
    "            raise ValueError(\"Each hidden layer must have a corresponding activation function\")\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activations = []\n",
    "\n",
    "        prev_size = input_size\n",
    "        for size, act in zip(hidden_layers, activations):\n",
    "            self.layers.append(nn.Linear(prev_size, size))\n",
    "            self.activations.append(activation_functions[act.lower()])\n",
    "            prev_size = size\n",
    "\n",
    "        # Output layer (1 node for binary classification)\n",
    "        self.output_layer = nn.Linear(prev_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer, activation in zip(self.layers, self.activations):\n",
    "            x = activation(layer(x))\n",
    "        x = self.output_layer(x)  # raw logits\n",
    "        return x.view(-1)  # flatten for BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f06382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Method for training the nn model for nth times based on the number of epochs specified \n",
    "def train_neural_network(model, train_loader, loss_function, optimizer, device, threshold, epochs):\n",
    "    # lists to store the loss and accuracy of each epoch\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # loop through each epoch\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training\", unit=\"epoch\"):\n",
    "        model.train() # Train the model\n",
    "        running_loss = 0.0 # Accumulate loss over each epoch\n",
    "        correct = 0 # Count of correct prediction\n",
    "        total = 0 # Total number of samples seen\n",
    "\n",
    "        # Iterate over batches from the training data loader\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device).view(-1)\n",
    "            optimizer.zero_grad() # Reset gradients\n",
    "            logits = model(batch_x).view(-1) # Forward Pass\n",
    "            loss = loss_function(logits, batch_y) # Compute loss\n",
    "            loss.backward() # Backpropagation\n",
    "            optimizer.step() # Update weights\n",
    "\n",
    "            running_loss += loss.item() * batch_x.size(0) # Accumulate weighted batch loss\n",
    "            preds = (torch.sigmoid(logits) >= threshold).float() # Get predicted classes\n",
    "            correct += (preds == batch_y).sum().item() # Count correct predictions\n",
    "            total += batch_y.size(0) # Update total samples\n",
    "\n",
    "        # Compute average loss and accuracy for the epoch\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_accuracy = correct / total\n",
    "        loss_history.append(epoch_loss)\n",
    "        accuracy_history.append(epoch_accuracy)\n",
    "\n",
    "        \n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_accuracy:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d12d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "optimizer_options = [\"adam\", \"sgd\", \"momentum\", \"nesterov\", \"adagrad\", \"adadelta\", \"rmsprop\"]\n",
    "\n",
    "def perform(df, train_loader, test_loader, input_size, output_size, hidden_layers, activations, pos_weight, threshold, epochs, optimizer_index, lr, momentum):\n",
    "\n",
    "    # Define model, loss function, and optimizer\n",
    "    # Instantiate model, loss, and optimizer\n",
    "    model = FraudClassifier(input_size=input_size, output_size=output_size, hidden_layers=hidden_layers, activations=activations)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Get optimizer name from list\n",
    "    optimizer_name = optimizer_options[optimizer_index].lower()\n",
    "\n",
    "    # Select Optimiser\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"momentum\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    elif optimizer_name == \"nesterov\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, nesterov=True)\n",
    "    elif optimizer_name == \"adagrad\":\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"adadelta\":\n",
    "        optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr, momentum=momentum)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
    "    \n",
    "    # Compute pos_weight as a Tensor\n",
    "    pos_weight_tensor = torch.tensor([pos_weight], dtype=torch.float).to(device)\n",
    "    \n",
    "    # Define the weighted loss\n",
    "    loss_function = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "    \n",
    "    # Train\n",
    "    fraud_nn = train_neural_network(\n",
    "        model,\n",
    "        train_loader,\n",
    "        loss_function,\n",
    "        optimizer,\n",
    "        device,\n",
    "        threshold,\n",
    "        epochs\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    fraud_nn.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    evaluate_nn_model(model, test_loader, device, threshold)\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
